# AI Fate Game ðŸŽ®

A Flask-based multiplayer survival game where players face AI-generated deadly scenarios and compete to survive!

## How It Works

1. **Create/Join Game**: One player creates a game and shares the Game ID with friends
2. **Random Selection**: Each round, a random player is chosen to pick one of 3 deadly scenarios generated by an AI (Llama model)
3. **Survival Planning**: All players write their survival plans for the chosen scenario
4. **AI Judgment**: Another AI evaluates each plan and determines who survives and who dies
5. **Multiple Rounds**: Play for a set number of rounds (default: 5)
6. **Winner**: The player who survives the most rounds wins!

## Prerequisites

- Python 3.8 or higher
- Ollama running with Llama model (or compatible API endpoint)

## Quick Start

### 1. Install Ollama (if not already installed)

Visit [https://ollama.ai](https://ollama.ai) and install Ollama, then pull a model:

```bash
ollama pull llama2
```

### 2. Clone and Setup

```bash
git clone https://github.com/BubblePlayzTHEREAL/AI-Fate-Game.git
cd AI-Fate-Game
pip install -r requirements.txt
```

### 3. Configure (Optional)

Copy `.env.example` to `.env` and customize:

```bash
cp .env.example .env
```

Edit `.env` to set:
- `LLAMA_TOPIC_IP`: IP address of Llama model for topic generation (default: http://localhost:11434)
- `LLAMA_JUDGE_IP`: IP address of Llama model for judging survival (default: http://localhost:11434)
- `MAX_ROUNDS`: Number of rounds per game (default: 5)
- `SECRET_KEY`: Flask secret key for sessions

### 4. Run the Game

```bash
python app.py
```

The game will be available at `http://localhost:5000`

## Game Features

- **Multiplayer Support**: Play with 2+ players
- **Real-time Updates**: Game state updates automatically
- **AI-Generated Scenarios**: Unique deadly situations each round
- **AI Judgment**: Fair evaluation of survival plans
- **Score Tracking**: Survival and death counts for each player
- **Responsive Design**: Works on desktop and mobile

## Configuration

You can configure the Llama model endpoints in two ways:

1. **Environment Variables** (`.env` file):
   ```
   LLAMA_TOPIC_IP=http://your-llama-server:11434
   LLAMA_JUDGE_IP=http://your-llama-server:11434
   ```

2. **Direct Configuration** (edit `config.py`):
   ```python
   LLAMA_TOPIC_IP = 'http://your-llama-server:11434'
   LLAMA_JUDGE_IP = 'http://your-llama-server:11434'
   ```

## Using Remote Llama Instances

If you want to use a remote Llama server:

1. Ensure the Llama API is accessible from your network
2. Set the `LLAMA_TOPIC_IP` and `LLAMA_JUDGE_IP` to the remote server URL
3. Make sure the API endpoints are compatible with Ollama's API format

## Game Rules

- Minimum 2 players to start
- Each round has 3 phases:
  1. **Topic Selection**: Random player chooses from 3 scenarios
  2. **Planning**: All players write survival plans
  3. **Judgment**: AI evaluates and announces results
- Players can see each other's survival/death counts
- Winner has the most survivals after all rounds

## Technical Details

- **Backend**: Flask (Python)
- **Frontend**: Vanilla JavaScript + CSS
- **AI Integration**: Ollama API (Llama models)
- **Game State**: In-memory storage (can be extended to database)

## Development

### Project Structure

```
AI-Fate-Game/
â”œâ”€â”€ app.py              # Main Flask application
â”œâ”€â”€ config.py           # Configuration settings
â”œâ”€â”€ llama_client.py     # Llama API client
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css   # Game styling
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ game.js     # Game logic
â””â”€â”€ templates/
    â”œâ”€â”€ base.html       # Base template
    â”œâ”€â”€ index.html      # Landing page
    â””â”€â”€ game.html       # Game interface
```

### Adding Features

Some ideas for extensions:
- Persistent storage (SQLite/PostgreSQL)
- User authentication
- Game history and statistics
- Different AI models for different difficulty levels
- Custom scenario categories
- Team-based gameplay

## Troubleshooting

**Issue**: "Error calling Llama model"
- **Solution**: Ensure Ollama is running and accessible at the configured IP

**Issue**: Topics are generic fallbacks
- **Solution**: Check Llama model is responding correctly, try pulling a different model

**Issue**: Game state lost on refresh
- **Solution**: This is expected with in-memory storage. Implement persistent storage for production use.

## License

MIT License - feel free to modify and use as you wish!

## Contributing

Pull requests are welcome! For major changes, please open an issue first to discuss what you would like to change.

## Credits

Built with Flask and powered by Llama AI models via Ollama.